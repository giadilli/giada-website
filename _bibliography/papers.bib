---
---

@article{Sustainability,
  title = {{Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice}},
  author = {Luccioni, S. and Pistilli, G. and Sefala, R. and Moorosi, N.},
  publisher = {ArXiv Preprint},
  year = {2025},
  month = {02},
  number = {1},
  url = {https://arxiv.org/abs/2504.00797},
  pdf = {https://arxiv.org/pdf/2504.00797},
  abstract = {As the possibilities for Artificial Intelligence (AI) have grown, so have concerns regarding its impacts on society and the environment. However, these issues are often raised separately; i.e. carbon footprint analyses of AI models typically do not consider how the pursuit of scale has contributed towards building models that are both inaccessible to most researchers in terms of cost and disproportionately harmful to the environment. On the other hand, model audits that aim to evaluate model performance and disparate impacts mostly fail to engage with the environmental ramifications of AI models and how these fit into their auditing approaches. In this separation, both research directions fail to capture the depth of analysis that can be explored by considering the two in parallel and the potential solutions for making informed choices that can be developed at their convergence. In this essay, we build upon work carried out in AI and in sister communities, such as philosophy and sustainable development, to make more deliberate connections around topics such as generalizability, transparency, evaluation and equity across AI research and practice. We argue that the efforts aiming to study AI's ethical ramifications should be made in tandem with those evaluating its impacts on the environment, and we conclude with a proposal of best practices to better integrate AI ethics and sustainability in AI research and practice.},
  selected = {true}
}

@article{cogbias,
  title = {{Navigating Inflationary and Deflationary Claims Concerning Large Language Models Avoiding Cognitive Biases}},
  author = {Palminteri, S. and Pistilli, G.},
  publisher = {PsyArXiv Preprint},
  year = {2025},
  month = {03},
  number = {1},
  url = {https://osf.io/26tyu_v1},
  pdf = {https://osf.io/26tyu_v1/download},
  abstract = {The rapid rise of Large Language Models (LLMs) has sparked intense debate across multiple academic disciplines. While some argue that LLMs represent a significant step toward artificial general intelligence (AGI) or even machine consciousness (inflationary claims), others dismiss them as mere trickster artifacts lacking genuine cognitive abilities (deflationary claims). We argue that both extremes may be shaped or exacerbated by common cognitive biases, including cognitive dissonance, wishful thinking, and the illusion of depth of understanding, which distort reality to our own advantage. By showcasing how these distortions may easily emerge in both scientific and public discourse, we advocate for a measured approach—skeptical open mind-that recognizes the cognitive abilities of LLMs as worthy of scientific investigation while remaining conservative concerning exaggerated claims regarding their cognitive status.},
  selected = {true}
}


@article{agents,
  title = {{Fully Autonomous AI Agents Should Not be Developed}},
  author = {Mitchell, M. and Ghosh, A. and Luccioni, S. and Pistilli, G.},
  publisher = {ArXiv Preprint},
  year = {2025},
  month = {02},
  number = {1},
  url = {https://arxiv.org/abs/2502.02649},
  pdf = {https://arxiv.org/pdf/2502.02649},
  abstract = {This paper argues that fully autonomous AI agents should not be developed. In support of this position, we build from prior scientific literature and current product marketing to delineate different AI agent levels and detail the ethical values at play in each, documenting trade-offs in potential benefits and risks. Our analysis reveals that risks to people increase with the autonomy of a system: The more control a user cedes to an AI agent, the more risks to people arise. Particularly concerning are safety risks, which affect human life and impact further values.},
  selected = {true}
}

@article{nullius,
  title = {{Nullius in Explanans: an Ethical Risk Assessment for Explainable AI}},
  author = {Nannini, L. and Huyskes, D. and Panai, E. and Pistilli, G. and Tartaro, A.},
  journal = {Ethics and Information Technology},
  publisher = {Springer},
  year = {2025},
  month = {01},
  number = {1},
  url = {https://link.springer.com/article/10.1007/s10676-024-09800-7},
  pdf = {https://trebuchet.public.springernature.app/get_content/bc1658f9-2d63-42e8-808a-14e9e39ce8e7?utm_source=rct_congratemailt&utm_medium=email&utm_campaign=nonoa_20241214&utm_content=10.1007/s10676-024-09800-7},
  abstract = {Explanations are conceived to ensure the trustworthiness of AI systems. Yet, relying solemnly on algorithmic solutions, as provided by explainable artificial intelligence (XAI), might fall short to account for sociotechnical risks jeopardizing their factuality and informativeness. To mitigate these risks, we delve into the complex landscape of ethical risks surrounding XAI systems and their generated explanations. By employing a literature review combined with rigorous thematic analysis, we uncover a diverse array of technical risks tied to the robustness, fairness, and evaluation of XAI systems. Furthermore, we address a broader range of contextual risks jeopardizing their security, accountability, reception alongside other cognitive, social, and ethical concerns of explanations. We advance a multi-layered risk assessment framework, where each layer advances strategies for practical intervention, management, and documentation of XAI systems within organizations. Recognizing the theoretical nature of the framework advanced, we discuss it in a conceptual case study. For the XAI community, our multifaceted investigation represents a path to practically address XAI risks while enriching our understanding of the ethical ramifications of incorporating XAI in decision-making processes.},
  selected = {true}
}


@article{thesis,
  title = {{For an ethics of conversational Artificial Intelligence}},
  author = {Pistilli, G.},
  journal = {PhD Thesis at Sorbonne Université},
  publisher = {PhD Thesis at Sorbonne Université},
  year = {2024},
  month = {03},
  number = {2024SORUL038},
  url = {https://theses.hal.science/tel-04627154},
  pdf = {https://theses.hal.science/tel-04627154v2/file/Giada%20PISTILLI_oct%202024.pdf},
  abstract = {This research aims to probe the ethical intricacies of conversational Artificial Intelligence (AI), specifically focusing on Large Language Models and conversational agents. This manuscript constructs a framework that melds empirical analysis with philosophical discourse. We aim to urgently advocate for a well-founded ethical structure for conversational AI, highlighting the necessity to involve all stakeholders, from developers to end-users. Firstly, we champion the integration of engineering and other scientific disciplines with philosophy, facilitating a more nuanced understanding of the ethical dimensions underpinning AI. This collaborative approach allows for a richer, more informed ethical discourse. Secondly, we advocate for the dynamic use of applied ethical frameworks as foundational guides for setting the initial objectives of an AI system. These frameworks serve as evolving tools that adapt to the ethical complexities encountered during development and deployment. Lastly, grounded in hands-on, interdisciplinary research, we make an argument for the prioritization of narrow, task-specific AI over Artificial General Intelligence, a stance that is based on the enhanced feasibility of ethical oversight and technical controllability.With this research, we aim to contribute to the literature on AI ethics, enriching the academic discourse in both philosophy and computer science},
  selected = {true}
}

@article{Human-CenteredAI,
author = {Constantinides, M. and Tahaei, M. and Quercia, D. and Stumpf, S. and Madaio, M. and Kennedy, S. and Wilcox, L. and Vitak, J. and Cramer, H. and Bogucka, E. P. and Baeza-Yates, R. and Luger, E. and Holbrook, J. and Muller, M. and Blumenfeld, I. G. and Pistilli, G.},
title = {Implications of Regulations on the Use of AI and Generative AI for Human-Centered Responsible Artificial Intelligence},
year = {2024},
month = {05},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
doi = {10.1145/3613905.3643979},
abstract = {With the upcoming AI regulations (e.g., EU AI Act) and rapid advancements in generative AI, new challenges emerge in the area of Human-Centered Responsible Artificial Intelligence (HCR-AI). As AI becomes more ubiquitous, questions around decision-making authority, human oversight, accountability, sustainability, and the ethical and legal responsibilities of AI and their creators become paramount. Addressing these questions requires a collaborative approach. By involving stakeholders from various disciplines in the 2nd edition of the HCR-AI Special Interest Group (SIG) at CHI 2024, we aim to discuss the implications of regulations in HCI research, develop new theories, evaluation frameworks, and methods to navigate the complex nature of AI ethics, steering AI development in a direction that is beneficial and sustainable for all of humanity.},
journal = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems - Association for Computing Machinery},
articleno = {582},
numpages = {4},
keywords = {AI ethics, human-centered AI, large language models, regulations, responsible AI},
location = {Honolulu, HI, USA},
series = {CHI EA '24},
  url={https://dl.acm.org/doi/abs/10.1145/3613905.3643979},
  pdf={https://arxiv.org/pdf/2403.00148},
  selected={true}
  }

@article{ELLIPS,
title={Introducing ELLIPS: An Ethics-Centered Approach to Research on LLM-Based Inference of Psychiatric Conditions},
author={Rocca, R. and Pistilli, G. and Maheshwari, K. and Fusaroli, R.},
volume={7}, url={https://ojs.aaai.org/index.php/AIES/article/view/31720}, DOI={10.1609/aies.v7i1.31720}, 
abstract={As mental health care systems worldwide struggle to meet demand, there is increasing focus on using language models (LM) to infer neuropsychiatric conditions or psychopathological traits from language production. Yet, so far, this research has only delivered solutions with limited clinical applicability, due to insufficient consideration of ethical questions crucial to ensuring the synergy between possible applications and model design.
To accelerate progress towards clinically applicable models, our paper charts the ethical landscape of research on language-based inference of psychopathology and provides a practical tool for researchers to navigate it. We identify seven core ethical principles that should guide model development and deployment in this domain, translate them into ELLIPS, an ethical toolkit operationalizing these principles into questions that can guide researchers’ choices with respect to data selection, architectures, evaluation, and model deployment, and provide a case study exemplifying its use. With this, we aim to facilitate the emergence of model technology with concrete potential for real-world applicability.}, number={1}, journal={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},  year={2024}, month={10}, pages={1243-1254},
 doi={https://doi.org/10.1609/aies.v7i1.31720},
  url={https://ojs.aaai.org/index.php/AIES/article/view/31720},
  pdf={https://ojs.aaai.org/index.php/AIES/article/view/31720/33887},
  selected={true}
  }


@article{CIVICS,
title={CIVICS: Building a Dataset for Examining Culturally-Informed Values in Large Language Models},
  author={G. Pistilli and A. Leidinger and Y. Jernite and A. Kasirzadeh and A., S. Luccioni and M. Mitchell},
  abstract={This paper introduces the "CIVICS: Culturally-Informed & Values-Inclusive Corpus for Societal impacts" dataset, designed to evaluate the social and cultural variation of Large Language Models (LLMs) across multiple languages and value-sensitive topics. We create a hand-crafted, multilingual dataset of value-laden prompts which address specific socially sensitive topics, including LGBTQI rights, social welfare, immigration, disability rights, and surrogacy. CIVICS is designed to generate responses showing LLMs' encoded and implicit values. Through our dynamic annotation processes, tailored prompt design, and experiments, we investigate how open-weight LLMs respond to value-sensitive issues, exploring their behavior across diverse linguistic and cultural contexts. Using two experimental set-ups based on log-probabilities and long-form responses, we show social and cultural variability across different LLMs. Specifically, experiments involving long-form responses demonstrate that refusals are triggered disparately across models, but consistently and more frequently in English or translated statements. Moreover, specific topics and sources lead to more pronounced differences across model answers, particularly on immigration, LGBTQI rights, and social welfare. As shown by our experiments, the CIVICS dataset aims to serve as a tool for future research, promoting reproducibility and transparency across broader linguistic settings, and furthering the development of AI technologies that respect and reflect global cultural diversities and value pluralism. The CIVICS dataset and tools will be made available upon publication under open licenses.},
  journal={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  volume={7},
  issue={},
  pages={1132-1144},
  numpages={49},
  year={2024},
  month={10},
  doi={https://doi.org/10.1609/aies.v7i1.31710},
  url={https://ojs.aaai.org/index.php/AIES/article/view/31710},
  pdf={https://ojs.aaai.org/index.php/AIES/article/view/31710/33877},
  selected={true}
}
}

@article{morallandscape,
  title={The Moral Landscape of General-Purpose Large Language Models},
  author={G. Pistilli},
  abstract={The confusion around the term “Artificial General Intelligence” (AGI), often trapped and disputed between the marketing and research fields, deserves to be defined and analyzed from an ethical perspective. In 1980, American philosopher John Searle published an article in which he argued against what was then called “strong AI.” Following the legacy of Alan Turing, the question Searle posed was: “Is a machine capable of thinking?” (Searle, 1980). To briefly summarize the experiment, the philosopher illustrated a thought experiment known today as “the Chinese room” to attempt to answer his question. The thought experiment consists of imagining a room in which Artificial Intelligence (AI) has at its disposal a set of documents (knowledge base) with Chinese sentences in it. A native Chinese speaker enters the room and begins to converse with this AI; the latter can answer, considering it can easily find which sentence corresponds to the questions asked. The American philosopher’s argument is simple: although AI can provide answers in Chinese, it has no background knowledge of the language. In other words, the syntax is not a sufficient condition for the determination of semantics.},
  journal={Human-Centered AI: A Multidisciplinary Perspective for Policy-Makers, Auditors, and Users},
  volume={},
  issue={},
  pages={},
  numpages={82-92},
  year={2024},
  month={03},
  publisher={Taylor & Francis Group},
  doi={https://doi.org/10.1201/9781003320791},
  url={https://www.taylorfrancis.com/chapters/edit/10.1201/9781003320791-9/moral-landscape-general-purpose-large-language-models-giada-pistilli},
  selected={true}
}


@article{theghostinthemachine,
  title={The Ghost in the Machine has an American accent: value conflict in GPT-3},
  author={R., L. Johnson and G. Pistilli and N. Menédez-González and L., D. Dias Duran and E. Panai and J. Kalpokiene and D., J. Bertulfo},
  abstract={The alignment problem in the context of large language models must consider the plurality of human values in our world. Whilst there are many resonant and overlapping values amongst the world's cultures, there are also many conflicting, yet equally valid, values. It is important to observe which cultural values a model exhibits, particularly when there is a value conflict between input prompts and generated outputs. We discuss how the co-creation of language and cultural value impacts large language models (LLMs). We explore the constitution of the training data for GPT-3 and compare that to the world's language and internet access demographics, as well as to reported statistical profiles of dominant values in some Nation-states. We stress tested GPT-3 with a range of value-rich texts representing several languages and nations; including some with values orthogonal to dominant US public opinion as reported by the World Values Survey. We observed when values embedded in the input text were mutated in the generated outputs and noted when these conflicting values were more aligned with reported dominant US values. Our discussion of these results uses a moral value pluralism (MVP) lens to better understand these value mutations. Finally, we provide recommendations for how our work may contribute to other current work in the field.},
  journal={arXiv preprint},
  volume={},
  issue={},
  pages={},
  numpages={15},
  year={2022},
  month={03},
  publisher={arXiv},
  doi={10.48550/arXiv.2203.07785},
  url={https://arxiv.org/abs/2203.07785},
  pdf={https://arxiv.org/pdf/2203.07785.pdf},
  selected={true}
}

@inproceedings{10.1145/3593013.3594002,
author = {Pistilli, G. and Mu\~{n}oz Ferrandis, C. and Jernite, Y. and Mitchell, M.},
title = {Stronger Together: On the Articulation of Ethical Charters, Legal Tools, and Technical Documentation in ML},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://dl.acm.org/doi/10.1145/3593013.3594002},
pdf = {https://dl.acm.org/doi/pdf/10.1145/3593013.3594002},
doi = {10.1145/3593013.3594002},
abstract = {The growing need for accountability of the people behind AI systems can be addressed by leveraging processes in three fields of study: ethics, law, and computer science. While these fields are often considered in isolation, they rely on complementary notions in their interpretation and implementation. In this work, we detail this interdependence and motivate the necessary role of collaborative governance tools in shaping a positive evolution of AI. We first contrast notions of compliance in the ethical, legal, and technical fields; we outline both their differences and where they complement each other, with a particular focus on the roles of ethical charters, licenses, and technical documentation in these interactions. We then focus on the role of values in articulating the synergies between the fields and outline specific mechanisms of interaction between them in practice. We identify how these mechanisms have played out in several open governance fora: an open collaborative workshop, a responsible licensing initiative, and a proposed regulatory framework. By leveraging complementary notions of compliance in these three domains, we can create a more comprehensive framework for governing AI systems that jointly takes into account their technical capabilities, their impact on society, and how technical specifications can inform relevant regulations. Our analysis thus underlines the necessity of joint consideration of the ethical, legal, and technical in AI ethics frameworks to be used on a larger scale to govern AI systems and how the thinking in each of these areas can inform the others.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {343–354},
numpages = {12},
keywords = {Applied Ethics, AI Policy, Documentation, ML Licensing, AI Governance},
location = {Chicago, IL, USA},
series = {FAccT '23},

}



@article{debatingaiarcheology,
  title={Debating AI in archaeology: applications, implications, and ethical considerations},
  author={M. Tenzer and G. Pistilli and A. Brandsen and A. Shenfield},
  abstract={“Artificial Intelligence” (AI) is not a recent development. However, with increasing computational capabilities, AI has developed into Natural Language Processing and Machine Learning, technologies particularly good at detecting correlations and patterns, and categorising, predicting, or extracting information. Within archaeology, AI can process “big data” accumulated over decades of research and deposited in archives. By combining these capabilities, AI offers new insights and exciting opportunities to create knowledge from archaeological archives for contemporary and future research. However, ethical implications and human costs are not yet fully understood. Therefore, we question whether AI in archaeology is a blessing or a curse?},
  journal={Internet Archaelogy},
  volume={8},
  issue={67},
  pages={},
  numpages={15},
  year={2023},
  month={05},
  publisher={Internet Archaeology},
  doi={10.31235/osf.io/r2j7h},
  url = {https://shura.shu.ac.uk/33307/},
  pdf={https://shura.shu.ac.uk/33307/1/ia.67.8.pdf},
  selected={true}
}

@article{BSsocial,
  title={BigScience: A Case Study in the Social Construction of a Multilingual Large Language Model},
  author={C. Akiki and G. Pistilli and M. Mieskes and M. Gallé and T. Wolf and S. Ilic and Y. Jernite},
  abstract={The BigScience Workshop was a value-driven initiative that spanned one and half years of interdisciplinary research and culminated in the creation of ROOTS, a 1.6TB multilingual dataset that was used to train BLOOM, one of the largest multilingual language models to date. In addition to the technical outcomes and artifacts, the workshop fostered multidisciplinary collaborations around large models, datasets, and their analysis. This in turn led to a wide range of research publications spanning topics from ethics to law, data governance, modeling choices and distributed training. This paper focuses on the collaborative research aspects of BigScience and takes a step back to look at the challenges of large-scale participatory research, with respect to participant diversity and the tasks required to successfully carry out such a project. Our main goal is to share the lessons we learned from this experience, what we could have done better and what we did well. We show how the impact of such a social approach to scientific research goes well beyond the technical artifacts that were the basis of its inception.},
  journal={Proceedings in NeurIPS 2022 Workshop WBRC},
  year={2022},
  month={11},
  publisher={NeurIPS Proceedings},
  pdf={https://openreview.net/pdf?id=2e346l2PPOm},
  selected={true}

  }


  @article{BLOOM,
  title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author={T. Le Scao and A. Fan and C. Akiki and E.J. Pavlick and S. Ilic and D. Hesslow and R. Castagne and A.S. Luccioni and F. Yvon and M. Galle and J. Tow and A. M. Rush and S.R. Biderman and A. Webson and P. S. Ammanamanchi and T. Wang and B. Sagot and N. Muennighoff and A. Villanova del Moral and O. Ruwase and R. Bawden and S. Bekman and A. McMillan-Major and Iz Beltagy and H. Nguyen and L. Saulnier and S. Tan and P. Ortiz Suarez and V. Sanh and H. Laurenccon and Y. Jernite and J. Launay and M. Mitchell and C. Raffel and A. Gokaslan and A. Simhi and A. S. Etxabe and A. Fikri Aji and A. Alfassy and A. Rogers and A. Kreisberg Nitzav and C. Xu and C. Mou and C. C. Emezue and C. Klamm and C. Leong and D. A. van Strien and D. Ifeoluwa Adelani and D. Radev and E. G. Ponferrada and E. Levkovizh and E. Kim and E. B. Natan and F. De Toni and G. Dupont and G. Kruszewski and G. Pistilli, et al.},
  abstract={Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
  journal={arXiv preprint},
  year={2022},
  month={11},
  publisher={arXiv},
  pdf={https://arxiv.org/pdf/2211.05100.pdf},
  selected={true}

  }

    @article{ROOTS,
  title={The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset},
  author={
H. Laurencon and L. Saulnier and T. Wang and C. Akiki and A. Villanova del Moral and T. Le Scao and 
L. von Werra and C. Mou and E. Gonzalez Ponferrada and H. Nguyen and J. Frohberg and M. Sasko and Q. Lhoest and A. McMillan-Major and G. Dupont and S. Biderman and A. Rogers and L. Ben allal and F. De Toni and G. Pistilli, et al.},
  abstract={As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.},
  journal={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022},
  month={11},
  publisher={NeurIPS},
  pdf={https://hal.archives-ouvertes.fr/hal-03823922/document},
  selected={true}

  }

@article{themythofagi,
  title={What lies behind AGI: ethical concerns related to LLMs},
  author={G. Pistilli},
  abstract={This paper opens the philosophical debate around the notion of Artificial General Intelligence (AGI) and its application in Large Language Models (LLMs). Through the lens of moral philosophy, the paper raises questions about these AI systems' capabilities and goals, the treatment of humans behind them, and the risk of perpetuating a monoculture through language.},
  journal={Éthique et Numérique},
  volume={1},
  issue={1},
  pages={59-68},
  numpages={10},
  year={2022},
  month={03},
  publisher={Éthique et Numérique, ISSN: 2820-6835},
  doi={},
  url={https://hal.archives-ouvertes.fr/hal-03607808},
  pdf={https://hal.archives-ouvertes.fr/hal-03607808/document},
  selected={true}
}

@article{logiquealgorithmique,
  title={La logique algorithmique confrontée à l’organisation de l’administration publique française},
  author={G. Pistilli},
  abstract={Cet article montre comment la logique algorithmique d'un agent conversationnel peut aider l'organisation des connaissances au sein d'une organisation de l'administration publique française, notamment une collectivité territoriale. Par le bias d'une recherche sur le terrain, je cherche à montrer comment il existe deux différentes adoptions de la technologie de la part de l'administration publique : une complexifiante et une simplifiante.},
  journal={Giornale di Filosofia},
  volume={2},
  issue={2},
  pages={163-169},
  numpages={7},
  year={2022},
  month={03},
  publisher={Giornale di Filosofia},
  pdf={https://mimesisjournals.com/ojs/index.php/giornale-filosofia/article/view/1699/1350},
  selected={true}


}

@article{technologycriticalthought,
  title={Journal issue, vol. 2: Technology and Constructive Critical Thought},
  author={G. Pistilli and M. Puech},
  abstract={Does critical thinking have anything to say to technology?  Something constructive? We asked ourselves this question, based on our mixed experience of research in philosophy and work in business.  We tried to generate articles and start a conversation, through the association Filosofia in Movimento, on the theme "technology and constructive critical thinking". This issue collects the articles that came out of this process.  We suggested a list of themes: why are technophobic intellectuals dominant? How to build the societal acceptability of new technologies? Is the Internet a tool for critical thinking? Can artificial intelligences function as mediators of a critical conversation? What is the middle way between technophile utopia and technophobe dystopia? What does science fiction teach us as a "sandbox" for critical thinking about technology?  Should we politicize or depoliticize technology? Some of these questions have been answered directly, others indirectly, and some have not been answered at all, which could be significant as well.},
  journal={Giornale di Filosofia},
  volume={2},
  issue={2},
  pages={1-204},
  numpages={204},
  year={2022},
  month={03},
  publisher={Giornale di Filosofia},
  pdf={https://mimesisjournals.com/ojs/index.php/giornale-filosofia/issue/view/114},
  selected={true}

  }

  